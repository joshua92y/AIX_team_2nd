# LocaAI/chatbot/core/rag_builder.py
import logging
from langchain_core.runnables import RunnableLambda, RunnableMap
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from chatbot.core.memory import DjangoChatHistory
from langchain.memory import ConversationBufferWindowMemory
from chatbot.models import ChatSession, ChatMemory, CollectionMemory, Prompt, ChatLog
from chatbot.utils.qdrant import get_collection_retriever, list_all_collections, get_embedding_model
from chatbot.utils.llm_config import get_llm
from django.conf import settings
from asgiref.sync import sync_to_async
from dataclasses import dataclass
from langchain_core.retrievers import BaseRetriever
from sklearn.metrics.pairwise import cosine_similarity


import json


logger = logging.getLogger(__name__)
async def load_all_prompts(collection_names: list[str]) -> dict[str, PromptTemplate]:
    prompt_objs = await sync_to_async(
        lambda: {p.name: p for p in Prompt.objects.filter(name__in=collection_names)}
    )()
    return {
        name: PromptTemplate.from_template(prompt_objs[name].content)
        for name in collection_names if name in prompt_objs
    }
@dataclass
class CollectionChainComponent:
    retriever: BaseRetriever
    prompt: PromptTemplate

def is_similar(summary, question, threshold=0.7):
    if not summary:
        return False
    embedding_model = get_embedding_model()
    summary_vec = [embedding_model.embed_query(summary)]
    question_vec = [embedding_model.embed_query(question)]
    sim = cosine_similarity(summary_vec, question_vec)[0][0]
    return sim >= threshold

# 1ï¸âƒ£ ìš”ì•½ ì‚½ì… ì²´ì¸
load_summary_chain = RunnableLambda(lambda inputs: {
    "question": (
        f"ì´ì „ ëŒ€í™” ì°¸ì¡°: {inputs['summary']} , ì§ˆë¬¸: {inputs['question']}"
        if is_similar(inputs.get("summary", ""), inputs['question'])
        else inputs['question']
    )
})
streaming_llm = get_llm(streaming=True)

# 2ï¸âƒ£ Qdrant ì»¬ë ‰ì…˜ë³„ ê²€ìƒ‰ + í”„ë¡¬í”„íŠ¸ ì ìš©
async def build_multi_collection_chain(
    collection_input_dict: dict[str, dict]
) -> RunnableMap:
    collection_names = list(collection_input_dict.keys())
    logger.debug(f"ğŸ” build_multi_collection_chain: {collection_names}")

    # âœ… ë¹„ë™ê¸° í”„ë¡¬í”„íŠ¸ ë¡œë”©
    prompts = await load_all_prompts(collection_names)
    # âœ… ë™ê¸° Retriever ìƒì„±
    retrievers = {name: get_collection_retriever(name) for name in collection_names}
    llm = get_llm()

    return RunnableMap({
        name: (
            RunnableLambda(lambda all_inputs, name=name: all_inputs[name]["question"])  # 1. ì§ˆë¬¸ë§Œ ì¶”ì¶œ
            .pipe(retrievers[name])  # 2. Vector ê²€ìƒ‰ (â†’ List[Document])

            # 3. ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì§ˆë¬¸ ê²°í•© (â†’ PromptTemplate ì…ë ¥ìš© dict + docs ë³´ì¡´)
            .pipe(RunnableLambda(lambda docs, name=name: {
                "question": collection_input_dict[name]["question"],
                "context": docs,
                "retrieved_docs": docs  # âœ… DB ì €ì¥ìš© ë¬¸ì„œ ë”°ë¡œ ë³´ì¡´
            }))

            # 4. PromptTemplate ì²˜ë¦¬
            .pipe(RunnableLambda(lambda inputs: {
                "formatted_prompt": prompts[name].invoke({
                    "question": inputs["question"],
                    "context": inputs["context"]
                }),
                "retrieved_docs": inputs["retrieved_docs"]
            }))

            # 5. LLM ì‹¤í–‰
            .pipe(RunnableLambda(lambda inputs: {
                "llm_response": llm.invoke(inputs["formatted_prompt"]),
                "retrieved_docs": inputs["retrieved_docs"]
            }))

            # 6. ìµœì¢… ì¶œë ¥ í…ìŠ¤íŠ¸ íŒŒì‹±
            .pipe(RunnableLambda(lambda result: {
                "llm_response": StrOutputParser().invoke(result["llm_response"]),
                "retrieved_docs": result["retrieved_docs"]
            }))
        )
        for name in collection_names
    })

# 3ï¸âƒ£ ì‘ë‹µ ì¡°í•© ì²´ì¸ (ì´ì œ build_combine_answers_chain í•¨ìˆ˜ë¡œ ëŒ€ì²´ë¨)

# 4ï¸âƒ£ ìš”ì•½ ì²´ì¸
summarize_chain = (
    RunnableLambda(lambda inputs: f"ì§ˆë¬¸: {inputs['question']}\në‹µë³€: {inputs['answer']}\nì¶œì²˜ ìš”ì•½: {inputs['collection_summary']}")
    | PromptTemplate.from_template("ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”:\n{input}")
    | get_llm()
    | StrOutputParser()
)

# 5ï¸âƒ£ DB ì €ì¥ ì²´ì¸
async def save_to_db_chain(user_id, session_id, question, answer, summary, collection_results):
    logger.debug("ğŸ’¾ save_to_db_chain ì‹œì‘")

    # âœ… ì„¸ì…˜ ë¡œë“œ
    session = await sync_to_async(ChatSession.objects.get)(user__id=user_id, session_id=session_id)

    # âœ… ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥
    await sync_to_async(ChatMemory.objects.create)(session=session, memory_type='question', role='user', content={"text": question})
    await sync_to_async(ChatMemory.objects.create)(session=session, memory_type='answer', role='assistant', content={"text": answer})
    await sync_to_async(ChatMemory.objects.create)(session=session, memory_type='summary', role='assistant', content={"text": summary})
    logger.debug("âœ… ChatMemory ì €ì¥ ì™„ë£Œ")

    # âœ… ì»¬ë ‰ì…˜ë³„ ë¬¸ì„œ ë° ì‘ë‹µ ì €ì¥
    for cname, result in collection_results.items():
        llm_response = result.get("llm_response")
        retrieved_docs = result.get("retrieved_docs", [])

        content_list = [doc.page_content for doc in retrieved_docs]
        meta_list = [doc.metadata for doc in retrieved_docs]

        prompt_obj = await sync_to_async(Prompt.objects.filter(name=cname).first)()

        await sync_to_async(CollectionMemory.objects.create)(
            session=session,
            collection_name=cname,
            retrieved_documents_content=content_list,
            retrieved_documents_meta=meta_list,
            llm_response=llm_response,
            prompt=prompt_obj
        )
        logger.debug(f"âœ… CollectionMemory ì €ì¥ ì™„ë£Œ: {cname}")

    # âœ… ëŒ€í™” ë¡œê·¸ ì €ì¥ (get_or_create ì´í›„ log ì´ˆê¸°í™” í™•ì¸)
    chatlog, _ = await sync_to_async(ChatLog.objects.get_or_create)(session=session)
    if not chatlog.log:
        chatlog.log = []
    chatlog.log += [
        {"role": "user", "content": question},
        {"role": "assistant", "content": answer}
    ]
    await sync_to_async(chatlog.save)()
    logger.debug("âœ… ChatLog ì €ì¥ ì™„ë£Œ")

# âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸
async def run_rag_pipeline(user_id: int, session_id: str, question: str, language: str = "ko"):
    logger.debug(f"ğŸš€ run_rag_pipeline ì‹œì‘ | user_id={user_id}, session_id={session_id}, language={language}, question={question[:30]}...")
    
    history = await DjangoChatHistory(user_id, session_id).load()
    memory = ConversationBufferWindowMemory(chat_memory=history, return_messages=True)

    recent_summary = await sync_to_async(lambda: ChatMemory.objects.filter(
        session__session_id=session_id, memory_type='summary'
    ).order_by('-created_at').first())()
    summary_text = recent_summary.content["text"] if recent_summary else ""
    logger.debug(f"ğŸ“Œ ë¶ˆëŸ¬ì˜¨ ìš”ì•½: {summary_text[:100]}...")

    # 1ï¸âƒ£ ìš”ì•½ ì‚½ì…
    question_with_summary = load_summary_chain.invoke({
        "summary": summary_text,
        "question": question
    })
    logger.debug(f"ğŸ” ìš”ì•½ì´ í¬í•¨ëœ ì§ˆë¬¸: {question_with_summary['question'][:100]}...")

    # 2ï¸âƒ£ ì»¬ë ‰ì…˜ë³„ ì‘ë‹µ
    allowed = set(settings.RAG_SETTINGS["COLLECTIONS"])
    collection_names = [name for name in list_all_collections() if name in allowed]
    logger.debug(f"ğŸ“š ëŒ€ìƒ ì»¬ë ‰ì…˜: {collection_names}")

    # âœ… PromptTemplateëŠ” dict input í•„ìš”
    collection_input_dict = {
        name: {
            "context": "",  # ì¶”í›„ í™•ì¥ ê°€ëŠ¥
            "chat_history": "",  # í•„ìš” ì‹œ history.get_messages()
            "question": question_with_summary["question"]
        } for name in collection_names
    }
    logger.debug(f"ğŸ§ª multi_chain ì…ë ¥ê°’: {json.dumps(collection_input_dict, indent=2, ensure_ascii=False)}")
    multi_chain = await build_multi_collection_chain(collection_input_dict)
    collection_answers = await multi_chain.ainvoke(collection_input_dict)
    logger.debug(f"ğŸ“¥ ì»¬ë ‰ì…˜ë³„ ì‘ë‹µ ì™„ë£Œ")

    # 3ï¸âƒ£ ì‘ë‹µ ì¡°í•©
    final_answer_chunks = []
    combine_answers_chain = await build_combine_answers_chain(language)
    async for chunk in combine_answers_chain.astream(collection_answers):
        final_answer_chunks.append(chunk)
        yield chunk

    final_answer = "".join(final_answer_chunks)
    logger.debug(f"âœ… ìµœì¢… ì‘ë‹µ ìƒì„± ì™„ë£Œ: {final_answer[:200]}...")
    history.add_ai_message(final_answer)

    # 4ï¸âƒ£ ìš”ì•½: collection_summaryëŠ” llm_responseë§Œ ì¶”ì¶œ
    collection_summary = "\n".join(
        result["llm_response"] for result in collection_answers.values()
    )

    summary = await summarize_chain.ainvoke({
        "question": question,
        "answer": final_answer,
        "collection_summary": collection_summary
    })
    logger.debug(f"ğŸ“ ìš”ì•½ ìƒì„± ì™„ë£Œ: {summary[:200]}...")

    # 5ï¸âƒ£ DB ì €ì¥
    await save_to_db_chain(
        user_id=user_id,
        session_id=session_id,
        question=question,
        answer=final_answer,
        summary=summary,
        collection_results=collection_answers  # dict[str, dict] êµ¬ì¡° ìœ ì§€
    )

    logger.debug("ğŸ¯ run_rag_pipeline ì™„ë£Œ")


# âœ… ì‘ë‹µ ì¡°í•© ì²´ì¸ (ì–¸ì–´ë³„)
async def build_combine_answers_chain(language: str = "ko"):
    from typing import Literal
    
    prompt_name = {
        "ko": "rag_combine_answers_ko",
        "en": "rag_combine_answers_en",
        "es": "rag_combine_answers_es"
    }.get(language, "rag_combine_answers_ko")

    try:
        prompt_obj = await sync_to_async(Prompt.objects.get)(name=prompt_name)
        prompt = PromptTemplate.from_template(prompt_obj.content)
        logger.info(f"âœ… Loaded prompt '{prompt_name}' from DB.")
    except Prompt.DoesNotExist:
        logger.error(f"âŒ Prompt '{prompt_name}' not found. Using fallback.")
        fallback_templates = {
            "ko": "ë‹¤ìŒì€ ì—¬ëŸ¬ ì¶œì²˜ì˜ ë‹µë³€ì…ë‹ˆë‹¤:\n\n{answers}\n\nìµœì¢… ìš”ì•½ëœ ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.",
            "en": "Based on the following multiple sources:\n\n{answers}\n\nPlease provide a final, summarized answer.",
            "es": "Basado en las siguientes fuentes:\n\n{answers}\n\nPor favor, proporciona una respuesta final resumida."
        }
        prompt = PromptTemplate.from_template(fallback_templates.get(language, fallback_templates["ko"]))

    def format_for_prompt(answers_dict: dict) -> dict:
        return {
            "answers": "\n\n".join(
                f"[{k}]\n{v.get('llm_response', 'No response.')}" for k, v in answers_dict.items()
            )
        }

    return (
        RunnableLambda(format_for_prompt)
        | prompt
        | streaming_llm
        | StrOutputParser()
    )


# âœ… LLM ì „ìš© íŒŒì´í”„ë¼ì¸
async def run_llm_pipeline(user_id: int, session_id: str, question: str, language: str = "ko"):
    logger.debug(f"ğŸš€ run_llm_pipeline ì‹œì‘ | user_id={user_id}, session_id={session_id}, lang={language}, question={question[:30]}...")

    # âœ… LLM í”„ë¡¬í”„íŠ¸ ë™ì  ë¡œë“œ (ì–¸ì–´ì— ë”°ë¼ ë¶„ê¸°) - LLM ì „ìš© í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
    if language == "ko":
        prompt_name = "llm_consultation"
    elif language == "en":
        prompt_name = "llm_consultation_en"
    elif language == "es":
        prompt_name = "llm_consultation_es"
    else:
        prompt_name = "llm_consultation" # ê¸°ë³¸ê°’

    try:
        prompt_obj = await sync_to_async(Prompt.objects.get)(name=prompt_name)
        llm_prompt = PromptTemplate.from_template(prompt_obj.content)
        logger.info(f"âœ… Loaded prompt '{prompt_name}' from DB for language '{language}'.")
    except Prompt.DoesNotExist:
        logger.error(f"âŒ CRITICAL: Prompt '{prompt_name}' not found in the database. This prompt is required for the LLM pipeline.")
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¡œ fallback - ìƒë‹´ AI ì—­í• 
        fallback_templates = {
            "ko": """ë‹¹ì‹ ì€ ìƒê¶Œ ë¶„ì„ ë° ì°½ì—… ìƒë‹´ ì „ë¬¸ AIì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ ì œê³µí•œ ì •ë³´ì™€ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì¸ ìƒë‹´ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ëŒ€í™” íˆìŠ¤í† ë¦¬:
{chat_history}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒê¶Œ ë¶„ì„, ì°½ì—…, ì‚¬ì—… ìš´ì˜ì— ëŒ€í•œ ì „ë¬¸ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•´ ì£¼ì„¸ìš”. êµ¬ì²´ì ì¸ ë°ì´í„°ë‚˜ ìˆ˜ì¹˜ê°€ ì—†ë”ë¼ë„ ì¼ë°˜ì ì¸ ì—…ê³„ ì§€ì‹ê³¼ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.""",
            "en": """You are a professional AI consultant specializing in commercial area analysis and business consulting. Please provide expert consultation based on the information and questions provided by the user.

Question: {question}

Chat History:
{chat_history}

Based on the above content, please provide professional and practical advice on commercial area analysis, business startup, and business operations. Even without specific data or figures, please give helpful answers based on general industry knowledge and experience.""",
            "es": """Eres un consultor de IA profesional especializado en anÃ¡lisis de Ã¡reas comerciales y consultorÃ­a empresarial. Proporciona consultorÃ­a experta basada en la informaciÃ³n y preguntas proporcionadas por el usuario.

Pregunta: {question}

Historial de chat:
{chat_history}

Basado en el contenido anterior, proporciona consejos profesionales y prÃ¡cticos sobre anÃ¡lisis de Ã¡reas comerciales, creaciÃ³n de empresas y operaciones comerciales. Incluso sin datos o cifras especÃ­ficas, da respuestas Ãºtiles basadas en conocimiento general de la industria y experiencia."""
        }
        llm_prompt = PromptTemplate.from_template(fallback_templates.get(language, fallback_templates["ko"]))

    # âœ… LLM ì²´ì¸ êµ¬ì„±
    llm_chain = (
        RunnableMap({
            "question": lambda x: x["question"],
            "chat_history": lambda x: x["chat_history"]
        })
        | llm_prompt
        | streaming_llm
        | StrOutputParser()
    )

    # âœ… 1. ëŒ€í™” íˆìŠ¤í† ë¦¬ ë° ìµœê·¼ ìš”ì•½ ë¡œë“œ
    history = await DjangoChatHistory(user_id, session_id).load()
    
    recent_summary = await sync_to_async(lambda: ChatMemory.objects.filter(
        session__session_id=session_id, memory_type='summary'
    ).order_by('-created_at').first())()
    summary_text = recent_summary.content["text"] if recent_summary else ""
    logger.debug(f"ğŸ“Œ [LLM] ë¶ˆëŸ¬ì˜¨ ìš”ì•½: {summary_text[:100]}...")

    # âœ… 2. ìš”ì•½ì„ ì§ˆë¬¸ì— í¬í•¨ (ìœ ì‚¬ë„ì— ë”°ë¼)
    question_with_summary = load_summary_chain.invoke({
        "summary": summary_text,
        "question": question
    })['question']
    logger.debug(f"ğŸ” [LLM] ìš”ì•½ í¬í•¨ëœ ì§ˆë¬¸: {question_with_summary[:100]}...")

    # âœ… 3. LLM ì²´ì¸ ì‹¤í–‰
    final_answer_chunks = []
    
    memory = ConversationBufferWindowMemory(chat_memory=history, return_messages=False, k=5)
    history_str = memory.load_memory_variables({}).get("history", "")

    async for chunk in llm_chain.astream({
        "question": question_with_summary,
        "chat_history": history_str
    }):
        final_answer_chunks.append(chunk)
        yield chunk

    final_answer = "".join(final_answer_chunks)
    logger.debug(f"âœ… [LLM] ìµœì¢… ì‘ë‹µ ìƒì„± ì™„ë£Œ: {final_answer[:200]}...")
    await sync_to_async(history.add_user_message)(question)
    await sync_to_async(history.add_ai_message)(final_answer)

    # âœ… 4. ëŒ€í™” ìš”ì•½ ìƒì„±
    summary = await summarize_chain.ainvoke({
        "question": question,
        "answer": final_answer,
        "collection_summary": ""  # RAGê°€ ì•„ë‹ˆë¯€ë¡œ ì¶œì²˜ ìš”ì•½ì€ ì—†ìŒ
    })
    logger.debug(f"ğŸ“ [LLM] ìš”ì•½ ìƒì„± ì™„ë£Œ: {summary[:200]}...")

    # âœ… 5. DB ì €ì¥
    await save_to_db_chain(
        user_id=user_id,
        session_id=session_id,
        question=question,
        answer=final_answer,
        summary=summary,
        collection_results={}  # RAGê°€ ì•„ë‹ˆë¯€ë¡œ ì»¬ë ‰ì…˜ ê²°ê³¼ëŠ” ì—†ìŒ
    )

    logger.debug("ğŸ¯ run_llm_pipeline ì™„ë£Œ")
