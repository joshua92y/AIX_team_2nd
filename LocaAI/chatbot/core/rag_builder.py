# LocaAI/chatbot/core/rag_builder.py
import logging
from langchain_core.runnables import RunnableLambda, RunnableMap
from langchain_core.prompts import PromptTemplate
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from chatbot.core.memory import DjangoChatHistory
from langchain.memory import ConversationBufferWindowMemory
from chatbot.models import ChatSession, ChatMemory, CollectionMemory, Prompt, ChatLog
from chatbot.utils.qdrant import get_collection_retriever, list_all_collections, get_embedding_model
from chatbot.utils.llm_config import get_llm
from django.conf import settings
from django.utils.timezone import now
from asgiref.sync import sync_to_async
from dataclasses import dataclass
from langchain_core.retrievers import BaseRetriever
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np


import json


logger = logging.getLogger(__name__)
async def load_all_prompts(collection_names: list[str]) -> dict[str, PromptTemplate]:
    prompt_objs = await sync_to_async(
        lambda: {p.name: p for p in Prompt.objects.filter(name__in=collection_names)}
    )()
    return {
        name: PromptTemplate.from_template(prompt_objs[name].content)
        for name in collection_names if name in prompt_objs
    }
@dataclass
class CollectionChainComponent:
    retriever: BaseRetriever
    prompt: PromptTemplate

def is_similar(summary, question, threshold=0.7):
    if not summary:
        return False
    embedding_model = get_embedding_model()
    summary_vec = [embedding_model.embed_query(summary)]
    question_vec = [embedding_model.embed_query(question)]
    sim = cosine_similarity(summary_vec, question_vec)[0][0]
    return sim >= threshold

# 1ï¸âƒ£ ìš”ì•½ ì‚½ì… ì²´ì¸
load_summary_chain = RunnableLambda(lambda inputs: {
    "question": (
        f"ì´ì „ ëŒ€í™” ì°¸ì¡°: {inputs['summary']} , ì§ˆë¬¸: {inputs['question']}"
        if is_similar(inputs.get("summary", ""), inputs['question'])
        else inputs['question']
    )
})
streaming_llm = get_llm(streaming=True)

# 2ï¸âƒ£ Qdrant ì»¬ë ‰ì…˜ë³„ ê²€ìƒ‰ + í”„ë¡¬í”„íŠ¸ ì ìš©
async def build_multi_collection_chain(
    collection_input_dict: dict[str, dict]
) -> RunnableMap:
    collection_names = list(collection_input_dict.keys())
    logger.debug(f"ğŸ” build_multi_collection_chain: {collection_names}")

    # âœ… ë¹„ë™ê¸° í”„ë¡¬í”„íŠ¸ ë¡œë”©
    prompts = await load_all_prompts(collection_names)
    # âœ… ë™ê¸° Retriever ìƒì„±
    retrievers = {name: get_collection_retriever(name) for name in collection_names}
    llm = get_llm()

    return RunnableMap({
        name: (
            RunnableLambda(lambda all_inputs, name=name: all_inputs[name]["question"])  # 1. ì§ˆë¬¸ë§Œ ì¶”ì¶œ
            .pipe(retrievers[name])  # 2. Vector ê²€ìƒ‰ (â†’ List[Document])

            # 3. ê²€ìƒ‰ëœ ë¬¸ì„œì™€ ì§ˆë¬¸ ê²°í•© (â†’ PromptTemplate ì…ë ¥ìš© dict + docs ë³´ì¡´)
            .pipe(RunnableLambda(lambda docs, name=name: {
                "question": collection_input_dict[name]["question"],
                "context": docs,
                "retrieved_docs": docs  # âœ… DB ì €ì¥ìš© ë¬¸ì„œ ë”°ë¡œ ë³´ì¡´
            }))

            # 4. PromptTemplate ì²˜ë¦¬
            .pipe(RunnableLambda(lambda inputs: {
                "formatted_prompt": prompts[name].invoke({
                    "question": inputs["question"],
                    "context": inputs["context"]
                }),
                "retrieved_docs": inputs["retrieved_docs"]
            }))

            # 5. LLM ì‹¤í–‰
            .pipe(RunnableLambda(lambda inputs: {
                "llm_response": llm.invoke(inputs["formatted_prompt"]),
                "retrieved_docs": inputs["retrieved_docs"]
            }))

            # 6. ìµœì¢… ì¶œë ¥ í…ìŠ¤íŠ¸ íŒŒì‹±
            .pipe(RunnableLambda(lambda result: {
                "llm_response": StrOutputParser().invoke(result["llm_response"]),
                "retrieved_docs": result["retrieved_docs"]
            }))
        )
        for name in collection_names
    })

# 3ï¸âƒ£ ì‘ë‹µ ì¡°í•© ì²´ì¸
combine_answers_chain = (
    RunnableLambda(lambda answers_dict: "\n\n".join(f"[{k}]\n{v}" for k, v in answers_dict.items()))
    | PromptTemplate.from_template("ë‹¤ìŒì€ ì—¬ëŸ¬ ì¶œì²˜ì˜ ë‹µë³€ì…ë‹ˆë‹¤:\n\n{answers}\n\nìµœì¢… ìš”ì•½ëœ ë‹µë³€ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.")
    | streaming_llm
    | StrOutputParser()
)

# 4ï¸âƒ£ ìš”ì•½ ì²´ì¸
summarize_chain = (
    RunnableLambda(lambda inputs: f"ì§ˆë¬¸: {inputs['question']}\në‹µë³€: {inputs['answer']}\nì¶œì²˜ ìš”ì•½: {inputs['collection_summary']}")
    | PromptTemplate.from_template("ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•´ ì£¼ì„¸ìš”:\n{input}")
    | get_llm()
    | StrOutputParser()
)

# 5ï¸âƒ£ DB ì €ì¥ ì²´ì¸
async def save_to_db_chain(user_id, session_id, question, answer, summary, collection_results):
    logger.debug("ğŸ’¾ save_to_db_chain ì‹œì‘")

    # âœ… ì„¸ì…˜ ë¡œë“œ
    session = await sync_to_async(ChatSession.objects.get)(user__id=user_id, session_id=session_id)

    # âœ… ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥
    await sync_to_async(ChatMemory.objects.create)(session=session, memory_type='question', role='user', content={"text": question})
    await sync_to_async(ChatMemory.objects.create)(session=session, memory_type='answer', role='assistant', content={"text": answer})
    await sync_to_async(ChatMemory.objects.create)(session=session, memory_type='summary', role='assistant', content={"text": summary})
    logger.debug("âœ… ChatMemory ì €ì¥ ì™„ë£Œ")

    # âœ… ì»¬ë ‰ì…˜ë³„ ë¬¸ì„œ ë° ì‘ë‹µ ì €ì¥
    for cname, result in collection_results.items():
        llm_response = result.get("llm_response")
        retrieved_docs = result.get("retrieved_docs", [])

        content_list = [doc.page_content for doc in retrieved_docs]
        meta_list = [doc.metadata for doc in retrieved_docs]

        prompt_obj = await sync_to_async(Prompt.objects.filter(name=cname).first)()

        await sync_to_async(CollectionMemory.objects.create)(
            session=session,
            collection_name=cname,
            retrieved_documents_content=content_list,
            retrieved_documents_meta=meta_list,
            llm_response=llm_response,
            prompt=prompt_obj
        )
        logger.debug(f"âœ… CollectionMemory ì €ì¥ ì™„ë£Œ: {cname}")

    # âœ… ëŒ€í™” ë¡œê·¸ ì €ì¥ (get_or_create ì´í›„ log ì´ˆê¸°í™” í™•ì¸)
    chatlog, _ = await sync_to_async(ChatLog.objects.get_or_create)(session=session)
    if not chatlog.log:
        chatlog.log = []
    chatlog.log += [
        {"role": "user", "content": question},
        {"role": "assistant", "content": answer}
    ]
    await sync_to_async(chatlog.save)()
    logger.debug("âœ… ChatLog ì €ì¥ ì™„ë£Œ")

# âœ… ì „ì²´ íŒŒì´í”„ë¼ì¸
async def run_rag_pipeline(user_id: int, session_id: str, question: str):
    logger.debug(f"ğŸš€ run_rag_pipeline ì‹œì‘ | user_id={user_id}, session_id={session_id}, question={question[:30]}...")
    
    history = await DjangoChatHistory(user_id, session_id).load()
    memory = ConversationBufferWindowMemory(chat_memory=history, return_messages=True)

    recent_summary = await sync_to_async(lambda: ChatMemory.objects.filter(
        session__session_id=session_id, memory_type='summary'
    ).order_by('-created_at').first())()
    summary_text = recent_summary.content["text"] if recent_summary else ""
    logger.debug(f"ğŸ“Œ ë¶ˆëŸ¬ì˜¨ ìš”ì•½: {summary_text[:100]}...")

    # 1ï¸âƒ£ ìš”ì•½ ì‚½ì…
    question_with_summary = load_summary_chain.invoke({
        "summary": summary_text,
        "question": question
    })
    logger.debug(f"ğŸ” ìš”ì•½ì´ í¬í•¨ëœ ì§ˆë¬¸: {question_with_summary['question'][:100]}...")

    # 2ï¸âƒ£ ì»¬ë ‰ì…˜ë³„ ì‘ë‹µ
    allowed = set(settings.RAG_SETTINGS["COLLECTIONS"])
    collection_names = [name for name in list_all_collections() if name in allowed]
    logger.debug(f"ğŸ“š ëŒ€ìƒ ì»¬ë ‰ì…˜: {collection_names}")

    # âœ… PromptTemplateëŠ” dict input í•„ìš”
    collection_input_dict = {
        name: {
            "context": "",  # ì¶”í›„ í™•ì¥ ê°€ëŠ¥
            "chat_history": "",  # í•„ìš” ì‹œ history.get_messages()
            "question": question_with_summary["question"]
        } for name in collection_names
    }
    logger.debug(f"ğŸ§ª multi_chain ì…ë ¥ê°’: {json.dumps(collection_input_dict, indent=2, ensure_ascii=False)}")
    multi_chain = await build_multi_collection_chain(collection_input_dict)
    collection_answers = await multi_chain.ainvoke(collection_input_dict)
    logger.debug(f"ğŸ“¥ ì»¬ë ‰ì…˜ë³„ ì‘ë‹µ ì™„ë£Œ")

    # 3ï¸âƒ£ ì‘ë‹µ ì¡°í•©
    final_answer_chunks = []
    async for chunk in combine_answers_chain.astream(collection_answers):
        final_answer_chunks.append(chunk)
        yield chunk

    final_answer = "".join(final_answer_chunks)
    logger.debug(f"âœ… ìµœì¢… ì‘ë‹µ ìƒì„± ì™„ë£Œ: {final_answer[:200]}...")
    history.add_ai_message(final_answer)

    # 4ï¸âƒ£ ìš”ì•½: collection_summaryëŠ” llm_responseë§Œ ì¶”ì¶œ
    collection_summary = "\n".join(
        result["llm_response"] for result in collection_answers.values()
    )

    summary = await summarize_chain.ainvoke({
        "question": question,
        "answer": final_answer,
        "collection_summary": collection_summary
    })
    logger.debug(f"ğŸ“ ìš”ì•½ ìƒì„± ì™„ë£Œ: {summary[:200]}...")

    # 5ï¸âƒ£ DB ì €ì¥
    await save_to_db_chain(
        user_id=user_id,
        session_id=session_id,
        question=question,
        answer=final_answer,
        summary=summary,
        collection_results=collection_answers  # dict[str, dict] êµ¬ì¡° ìœ ì§€
    )

    logger.debug("ğŸ¯ run_rag_pipeline ì™„ë£Œ")
