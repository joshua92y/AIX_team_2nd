# apps/chatbot/rag_pipeline.py
import os
import json
import hashlib
import asyncio
import logging
from typing import List, Dict, Any

from dotenv import load_dotenv
from django.conf import settings
from django.core.cache import cache
from functools import partial, lru_cache

from langchain_qdrant import Qdrant
from langchain_openai import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain_core.runnables import RunnableParallel, RunnableLambda
from langchain_huggingface import HuggingFaceEmbeddings
from qdrant_client import QdrantClient
from qdrant_client.http import models
from langchain_core.prompts import PromptTemplate
from .memory import DjangoConversationMemory
from langchain.schema import BaseRetriever, Document
from langchain.callbacks.manager import CallbackManagerForRetrieverRun
from pydantic import Field

logger = logging.getLogger(__name__)

# âœ… í™˜ê²½ë³€ìˆ˜ ë¡œë”©
load_dotenv(dotenv_path=os.path.join(os.path.dirname(__file__), "..", ".env"))

# âœ… ì„¤ì •
llm = ChatOpenAI(streaming=True, model="gpt-4o-mini")
embedding_model = HuggingFaceEmbeddings(
    model_name=settings.RAG_SETTINGS["EMBEDDING_MODEL"],
    model_kwargs={"device": "cpu"},
    encode_kwargs={"normalize_embeddings": True},
)

# ì „ì—­ ë³€ìˆ˜ë¡œ ì²´ì¸ê³¼ ë©”ëª¨ë¦¬ ì •ì˜
final_chain = None
summary_memory = None  # ì´ˆê¸°í™”ëŠ” ë‚˜ì¤‘ì— ìˆ˜í–‰


class CustomQdrantRetriever(BaseRetriever):
    client: Any
    collection_name: str
    embed_model: Any

    def get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun = None
    ) -> list[Document]:
        vector = self.embed_model.embed_query(query)
        results = self.client.search(
            collection_name=self.collection_name,
            query_vector=vector,
            limit=5,
            with_payload=True,
            with_vectors=False,
        )
        docs = []
        for r in results:
            payload = r.payload
            content = payload.get("text") or payload.get("page_content", "")
            metadata = {
                k: v for k, v in payload.items() if k not in ["text", "page_content"]
            }
            docs.append(Document(page_content=content, metadata=metadata))
        return docs


# âœ… RAG ì‘ë‹µ DB ì €ì¥
def make_rag_saver(user_id, session_id):
    from .models import ConversationLog

    def save_rag_outputs(rag_outputs):
        zone_answers = {
            "rag_zone_1_answer": rag_outputs.get("rag_1", {}).get("answer", ""),
            # "rag_zone_2_answer": rag_outputs.get("rag_2", {}).get("answer", ""),
            # "rag_zone_3_answer": rag_outputs.get("rag_3", {}).get("answer", ""),
        }
        # ë¡œê·¸ ì €ì¥
        ConversationLog.objects.create(
            user_id=user_id,
            session_id=session_id,
            role="assistant",
            content=format_all_rag_answers(rag_outputs),
            **zone_answers,
        )
        return rag_outputs

    return save_rag_outputs


# âœ… RAG ì‘ë‹µ í¬ë§·í„°
def format_all_rag_answers(results_dict: dict) -> str:
    return "\n\n".join(f"[{k}]\n{v['answer']}" for k, v in results_dict.items())


# âœ… ìºì‹± ìœ í‹¸
async def get_cache_key(input_data: dict) -> str:
    data_str = json.dumps(input_data, sort_keys=True)
    return f"rag_cache_{hashlib.md5(data_str.encode()).hexdigest()}"


async def cached_rag_call(chain, input_data: dict, cache_timeout: int = 3600):
    logger.debug("â–¶ï¸ cached_rag_call ì‹œì‘")
    cache_key = await get_cache_key(input_data)
    logger.debug(f"ğŸ”‘ ìºì‹œ í‚¤: {cache_key}")

    cached_result = await asyncio.to_thread(cache.get, cache_key)
    if cached_result:
        logger.debug("âœ… ìºì‹œëœ ê²°ê³¼ ë°˜í™˜")
        return cached_result

    logger.debug("ğŸš€ ìºì‹œ ë¯¸ì¡´ì¬ â†’ chain.invoke ìˆ˜í–‰")
    result = await asyncio.to_thread(chain.invoke, input_data)
    await asyncio.to_thread(cache.set, cache_key, result, cache_timeout)
    logger.debug("ğŸ“¦ ê²°ê³¼ ìºì‹œì— ì €ì¥ ì™„ë£Œ")
    return result


# âœ… ìµœì¢… RAG ì²´ì¸ ìƒì„±ê¸°
async def build_final_chain(user_id=None, session_id=None):
    logger.debug("â–¶ï¸ build_final_chain ì‹œì‘")
    if user_id is None:
        user_id = "default_user"
    if session_id is None:
        session_id = "default_session"

    logger.debug(f"ğŸ‘¤ user_id: {user_id}, session_id: {session_id}")

    buffer_memory = DjangoConversationMemory(
        user_id=user_id, session_id=session_id, k=settings.RAG_SETTINGS["MEMORY_K"]
    )
    summary_memory = DjangoConversationMemory(
        user_id=user_id, session_id=session_id, summary=True
    )
    logger.debug("ğŸ§  ë©”ëª¨ë¦¬ ê°ì²´ ìƒì„± ì™„ë£Œ")

    async def build_rag_chain(collection_name: str):
        logger.debug(f"ğŸ”— build_rag_chain: {collection_name} ì‹œì‘")
        try:
            client = QdrantClient(
                url=settings.RAG_SETTINGS["QDRANT_URL"],
                api_key=settings.RAG_SETTINGS["QDRANT_API_KEY"],
            )

            retriever = CustomQdrantRetriever(
                client=client,
                collection_name=collection_name,
                embed_model=embedding_model,
            )
            logger.debug(f"âœ… {collection_name} custom retriever ìƒì„± ì™„ë£Œ")

            try:
                test_docs = retriever.get_relevant_documents("ì´ì¸êµ¬")
                logger.debug(f"ğŸ“š Qdrant {collection_name} ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ (ìƒ˜í”Œ):")
                for i, doc in enumerate(test_docs[:3]):
                    logger.debug(f"  - [{i+1}] ë©”íƒ€ë°ì´í„°: {doc.metadata}")
                    logger.debug(f"  - [{i+1}] ë‚´ìš©: {repr(doc.page_content)[:120]}...")
            except Exception as doc_e:
                logger.warning(f"âš ï¸ {collection_name}ì—ì„œ ë¬¸ì„œ ë¯¸ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {doc_e}")

            return ConversationalRetrievalChain.from_llm(
                llm=llm,
                retriever=retriever,
                memory=buffer_memory,
                return_source_documents=False,
            )

        except Exception as e:
            logger.error(f"âŒ {collection_name} RAG ì²´ì¸ ìƒì„± ì‹¤íŒ¨", exc_info=True)
            return None

    rag_chains = {}
    for i, name in enumerate(settings.RAG_SETTINGS["COLLECTIONS"]):
        chain = await build_rag_chain(name)
        if chain:
            key = f"rag_{i+1}"
            rag_chains[key] = chain
            logger.debug(f"âœ… ë³‘ë ¬ ì²´ì¸ ì¶”ê°€: {key}")
        else:
            logger.warning(f"âš ï¸ {name} ì²´ì¸ ìƒì„± ì‹¤íŒ¨ â†’ ë³‘ë ¬ êµ¬ì„±ì—ì„œ ì œì™¸")

    parallel_rag = RunnableParallel(rag_chains)
    logger.debug("ğŸ”€ ë³‘ë ¬ ì²´ì¸ êµ¬ì„± ì™„ë£Œ")

    def insert_summary_prompt(input_dict):
        question = input_dict["question"]
        summary = summary_memory.get_summary() or "(ìš”ì•½ ì—†ìŒ)"
        logger.debug(f"ğŸ“‹ ìš”ì•½ ì‚½ì…: {summary}")
        return {"question": f"[ëŒ€í™” ìš”ì•½]\n{summary}\n\n[ì‚¬ìš©ì ì§ˆë¬¸]\n{question}"}

    saver = make_rag_saver(user_id, session_id)
    logger.debug("ğŸ’¾ ì €ì¥ ë¡œì§ ìƒì„± ì™„ë£Œ")

    chain = (
        RunnableLambda(insert_summary_prompt)
        .pipe(parallel_rag)
        .pipe(RunnableLambda(saver))
        .pipe(RunnableLambda(format_all_rag_answers))
        .pipe(llm)
    )

    logger.debug("âœ… ìµœì¢… RAG ì²´ì¸ ìƒì„± ì™„ë£Œ")
    return lambda x: cached_rag_call(chain, x)


async def initialize_chains():
    logger.debug("â–¶ï¸ initialize_chains ì‹œì‘")
    global final_chain, summary_memory

    try:
        embeddings = HuggingFaceEmbeddings(
            model_name=settings.RAG_SETTINGS["EMBEDDING_MODEL"],
            model_kwargs={"device": "cpu"},
            encode_kwargs={"normalize_embeddings": True},
        )
        logger.debug("ğŸ”§ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

        client = QdrantClient(
            url=settings.RAG_SETTINGS["QDRANT_URL"],
            api_key=settings.RAG_SETTINGS["QDRANT_API_KEY"],
        )
        logger.debug("ğŸ”— Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

        vectorstore = Qdrant(
            client=client,
            collection_name=settings.RAG_SETTINGS["COLLECTIONS"][0],
            embeddings=embeddings,
        )
        logger.debug("ğŸ“¦ Qdrant ë²¡í„°ìŠ¤í† ì–´ ì—°ê²° ì™„ë£Œ")

        memory = DjangoConversationMemory(
            user_id="default_user",
            session_id="default_session",
            k=settings.RAG_SETTINGS["MEMORY_K"],
        )
        logger.debug("ğŸ§  ê¸°ë³¸ ë©”ëª¨ë¦¬ êµ¬ì„± ì™„ë£Œ")

        template = """ë‹¹ì‹ ì€ í•œêµ­ì˜ ìƒê¶Œê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ë˜, ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ëŠ” ì œê³µí•˜ì§€ ë§ˆì„¸ìš”.
        ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì œê³µí•´ì£¼ì„¸ìš”.

        ì»¨í…ìŠ¤íŠ¸: {context}

        ì´ì „ ëŒ€í™”:
        {chat_history}

        ì§ˆë¬¸: {question}

        ë‹µë³€:"""

        QA_CHAIN_PROMPT = PromptTemplate(
            input_variables=["context", "chat_history", "question"],
            template=template,
        )
        logger.debug("ğŸ“ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • ì™„ë£Œ")

        final_chain = ConversationalRetrievalChain.from_llm(
            llm=ChatOpenAI(
                model_name="gpt-4o-mini",
                temperature=0,
                streaming=True,
            ),
            retriever=vectorstore.as_retriever(
                search_kwargs={"k": settings.RAG_SETTINGS["RETRIEVER_K"]}
            ),
            memory=memory,
            combine_docs_chain_kwargs={"prompt": QA_CHAIN_PROMPT},
            return_source_documents=True,
            return_generated_question=True,
        )
        logger.info("âœ… ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ")
        return True

    except Exception as e:
        logger.error("âŒ initialize_chains ì‹¤íŒ¨", exc_info=True)
        return False


# ì´ˆê¸°í™”ëŠ” Django ì•±ì´ ì‹œì‘ë  ë•Œ ìˆ˜í–‰ë¨
