# LocaAI/chatbot/utils/llm_config.py

from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel
from django.conf import settings
import os


def get_llm(streaming: bool = False) -> BaseChatModel:
    """RAG_SETTINGSì— ì§€ì •ëœ LLM ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬ ë°˜í™˜"""
    model_name = settings.RAG_SETTINGS.get("LLM_MODEL", "gpt-4o-mini")
    api_key = os.getenv("OPENAI_API_KEY")

    if model_name.startswith("gpt-"):
        return ChatOpenAI(
            model=model_name,
            temperature=0.7,
            streaming=streaming,
            openai_api_key=api_key
        )
    
    # ğŸ’¡ Claude, Gemini ë“± ë‹¤ë¥¸ ëª¨ë¸ í™•ì¥ ëŒ€ë¹„ êµ¬ì¡°
    raise ValueError(f"LLM model `{model_name}` is not supported yet.")
