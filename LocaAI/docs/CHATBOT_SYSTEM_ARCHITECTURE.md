# LocaAI ì±—ë´‡ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

## ğŸ“‹ ì‹œìŠ¤í…œ ê°œìš”

LocaAI ì±—ë´‡ ì‹œìŠ¤í…œì€ **ìƒê¶Œ ë¶„ì„ ë° ì°½ì—… ìƒë‹´ ì „ë¬¸ AI**ë¡œ, ë‘ ê°€ì§€ ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤:

- **ğŸ§  LLM ëª¨ë“œ**: ìˆœìˆ˜ ì–¸ì–´ëª¨ë¸ ê¸°ë°˜ ìƒë‹´
- **ğŸ—„ï¸ RAG ëª¨ë“œ**: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ ì •ë³´ ê²€ìƒ‰ + ìƒì„±

## ğŸ—ï¸ ì „ì²´ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "Frontend"
        UI[ì›¹ ì¸í„°í˜ì´ìŠ¤]
        Toggle[LLM/RAG í† ê¸€]
        Chat[ì±„íŒ… UI]
    end

    subgraph "Backend"
        WS[WebSocket Consumer]
        Router{ëª¨ë“œ ë¶„ê¸°}
        
        subgraph "LLM Pipeline"
            LLM_Load[í”„ë¡¬í”„íŠ¸ ë¡œë“œ]
            LLM_History[íˆìŠ¤í† ë¦¬ ì²˜ë¦¬]
            LLM_Chain[LLM ì²´ì¸ ì‹¤í–‰]
            LLM_Stream[ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°]
        end
        
        subgraph "RAG Pipeline"
            RAG_Load[í”„ë¡¬í”„íŠ¸ ë¡œë“œ]
            RAG_Search[ë²¡í„° ê²€ìƒ‰]
            RAG_Multi[ë‹¤ì¤‘ ì»¬ë ‰ì…˜ ì²˜ë¦¬]
            RAG_Combine[ì‘ë‹µ ì¡°í•©]
            RAG_Stream[ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°]
        end
        
        subgraph "Storage"
            DB[(PostgreSQL)]
            Vector[(Qdrant)]
        end
    end

    UI --> WS
    Toggle --> WS
    WS --> Router
    
    Router -->|mode="llm"| LLM_Load
    Router -->|mode="rag"| RAG_Load
    
    LLM_Load --> LLM_History --> LLM_Chain --> LLM_Stream
    RAG_Load --> RAG_Search --> RAG_Multi --> RAG_Combine --> RAG_Stream
    
    LLM_Stream --> Chat
    RAG_Stream --> Chat
    
    LLM_Chain -.-> DB
    RAG_Search -.-> Vector
    RAG_Multi -.-> DB
```

## ğŸ”€ ëª¨ë“œ ë¶„ê¸° ì‹œìŠ¤í…œ

### 1. í”„ë¡ íŠ¸ì—”ë“œ ëª¨ë“œ ì„ íƒ

**ìœ„ì¹˜**: `static/js/chat_main.js`, `static/js/ai_analyzer/analyze-chatbot.js`

```javascript
function getCurrentMode() {
  const llmMode = document.getElementById("llmMode");
  const ragMode = document.getElementById("ragMode");
  return llmMode && llmMode.checked ? "llm" : 
         (ragMode && ragMode.checked ? "rag" : "llm");
}

// WebSocket ë©”ì‹œì§€ ì „ì†¡ ì‹œ ëª¨ë“œ í¬í•¨
const payload = { 
  user_id: userId, 
  session_id: currentSessionId, 
  question: msg,
  mode: selectedMode,  // ğŸ”‘ í•µì‹¬: ëª¨ë“œ íŒŒë¼ë¯¸í„°
  language: "ko"
};
```

### 2. ë°±ì—”ë“œ ëª¨ë“œ ì²˜ë¦¬

**ìœ„ì¹˜**: `chatbot/consumers.py`

```python
async def receive(self, text_data):
    data = json.loads(text_data)
    mode = data.get("mode", "llm")  # ê¸°ë³¸ê°’: LLM
    language = data.get("language", "ko")  # ê¸°ë³¸ê°’: í•œêµ­ì–´
    
    # ğŸš¦ ëª¨ë“œì— ë”°ë¥¸ ë¶„ê¸°
    if mode == "rag":
        logger.info("ğŸ—„ï¸ RAG ëª¨ë“œë¡œ ì²˜ë¦¬ ì‹œì‘")
        async for chunk in run_rag_pipeline(
            self.user_id, self.session_id, question, language
        ):
            await self.send(text_data=json.dumps({"chunk": chunk}))
    else:  # llm
        logger.info("ğŸ§  LLM ëª¨ë“œë¡œ ì²˜ë¦¬ ì‹œì‘")
        async for chunk in run_llm_pipeline(
            self.user_id, self.session_id, question, language
        ):
            await self.send(text_data=json.dumps({"chunk": chunk}))
```

## ğŸ§  LLM ëª¨ë“œ íŒŒì´í”„ë¼ì¸

### ì²˜ë¦¬ íë¦„

**ìœ„ì¹˜**: `chatbot/core/rag_builder.py:run_llm_pipeline()`

1. **ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ ë¡œë“œ**: DBì—ì„œ `llm_consultation_ko/en/es` ì¡°íšŒ
2. **ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¡œë“œ**: ê¸°ì¡´ ëŒ€í™” ë° ìš”ì•½ ë¶ˆëŸ¬ì˜¤ê¸°
3. **ìš”ì•½+ì§ˆë¬¸ ê²°í•©**: ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ ì´ì „ ìš”ì•½ í¬í•¨ ì—¬ë¶€ ê²°ì •
4. **LLM ì²´ì¸ ì‹¤í–‰**: GPT-4o-minië¡œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
5. **ê²°ê³¼ ì €ì¥**: ëŒ€í™”, ì‘ë‹µ, ìš”ì•½ì„ DBì— ì €ì¥

### ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ ë¶„ê¸°

```python
# ì–¸ì–´ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì„ íƒ
if language == "ko":
    prompt_name = "llm_consultation"
elif language == "en":
    prompt_name = "llm_consultation_en"  
elif language == "es":
    prompt_name = "llm_consultation_es"
else:
    prompt_name = "llm_consultation"  # ê¸°ë³¸ê°’

# DBì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
prompt_obj = await sync_to_async(Prompt.objects.get)(name=prompt_name)
llm_prompt = PromptTemplate.from_template(prompt_obj.content)
```

## ğŸ—„ï¸ RAG ëª¨ë“œ íŒŒì´í”„ë¼ì¸

### ì²˜ë¦¬ íë¦„

**ìœ„ì¹˜**: `chatbot/core/rag_builder.py:run_rag_pipeline()`

1. **ìš”ì•½+ì§ˆë¬¸ ê²°í•©**: ì´ì „ ëŒ€í™” ìš”ì•½ê³¼ í˜„ì¬ ì§ˆë¬¸ ê²°í•©
2. **ë‹¤ì¤‘ ì»¬ë ‰ì…˜ ê²€ìƒ‰**: í—ˆìš©ëœ ì»¬ë ‰ì…˜ë“¤ì—ì„œ ë³‘ë ¬ ë²¡í„° ê²€ìƒ‰
3. **ì»¬ë ‰ì…˜ë³„ ì‘ë‹µ ìƒì„±**: ê° ì»¬ë ‰ì…˜ë§ˆë‹¤ ë…ë¦½ì ì¸ LLM ì‘ë‹µ
4. **ì‘ë‹µ ì¡°í•©**: ì–¸ì–´ë³„ ì¡°í•© í”„ë¡¬í”„íŠ¸ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
5. **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**: ì¡°í•©ëœ ì‘ë‹µì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì „ì†¡
6. **ê²°ê³¼ ì €ì¥**: ê²€ìƒ‰ ë¬¸ì„œ, ì‘ë‹µ, ìš”ì•½ ë“± ëª¨ë“  ë°ì´í„° ì €ì¥

### ë‹¤ì¤‘ ì»¬ë ‰ì…˜ ì²˜ë¦¬

**ì„¤ì •**: `chatbot/rag_settings.py`

```python
RAG_SETTINGS = {
    "COLLECTIONS": [
        "workpeople_zones",                    # ì§ì¥ì¸êµ¬ ë°ì´í„°
        "analysis_result_consultation",       # ìƒê¶Œë¶„ì„ ìƒë‹´ (í•œêµ­ì–´)
        "analysis_result_consultation_en",    # ìƒê¶Œë¶„ì„ ìƒë‹´ (ì˜ì–´)  
        "analysis_result_consultation_es"     # ìƒê¶Œë¶„ì„ ìƒë‹´ (ìŠ¤í˜ì¸ì–´)
    ],
    "EMBEDDING_MODEL": "upskyy/bge-m3-korean",
    "LLM_MODEL": "gpt-4o-mini",
    "RETRIEVER_K": 3  # ì»¬ë ‰ì…˜ë³„ ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜
}
```

## ğŸŒ ë‹¤êµ­ì–´ ì²˜ë¦¬ ì‹œìŠ¤í…œ

### 1. ì–¸ì–´ ê°ì§€ ë° ì „ì†¡

**í”„ë¡ íŠ¸ì—”ë“œ**:
```javascript
// AI_Analyzerì—ì„œ ì–¸ì–´ ê°ì§€
function getCurrentLanguage() {
    const korBtn = document.querySelector('.lang-btn[data-lang="KOR"]');
    const engBtn = document.querySelector('.lang-btn[data-lang="ENG"]'); 
    const espBtn = document.querySelector('.lang-btn[data-lang="ESP"]');
    
    if (korBtn && korBtn.classList.contains('active')) return 'ko';
    if (engBtn && engBtn.classList.contains('active')) return 'en';  
    if (espBtn && espBtn.classList.contains('active')) return 'es';
    return 'ko'; // ê¸°ë³¸ê°’
}

// ì»¬ë ‰ì…˜ ì´ë¦„ ë§¤í•‘
function getCollectionNameByLanguage(language) {
    const mapping = {
        'ko': 'analysis_result_consultation',
        'en': 'analysis_result_consultation_en', 
        'es': 'analysis_result_consultation_es'
    };
    return mapping[language] || mapping['ko'];
}
```

### 2. ë°±ì—”ë“œ ì–¸ì–´ ì²˜ë¦¬

**RAG ëª¨ë“œ - ì‘ë‹µ ì¡°í•© í”„ë¡¬í”„íŠ¸**:
```python
async def build_combine_answers_chain(language: str = "ko"):
    prompt_name = {
        "ko": "rag_combine_answers_ko",
        "en": "rag_combine_answers_en", 
        "es": "rag_combine_answers_es"
    }.get(language, "rag_combine_answers_ko")
    
    # DBì—ì„œ ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    prompt_obj = await sync_to_async(Prompt.objects.get)(name=prompt_name)
    prompt = PromptTemplate.from_template(prompt_obj.content)
```

## ğŸ”§ í•µì‹¬ ì»´í¬ë„ŒíŠ¸

### 1. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°

**ìœ„ì¹˜**: `chatbot/utils/qdrant.py`

```python
@lru_cache
def get_qdrant_client() -> QdrantClient:
    return QdrantClient(url=settings.QDRANT_URL, api_key=settings.QDRANT_API_KEY)

@lru_cache
def get_embedding_model() -> HuggingFaceEmbeddings:
    return HuggingFaceEmbeddings(
        model_name="upskyy/bge-m3-korean",  # ë‹¤êµ­ì–´ ì§€ì› ì„ë² ë”©
        model_kwargs={"device": "cuda" if settings.USE_CUDA else "cpu"}
    )
```

### 2. LLM ì„¤ì •

**ìœ„ì¹˜**: `chatbot/utils/llm_config.py`

```python
def get_llm(streaming: bool = False) -> BaseChatModel:
    model_name = settings.RAG_SETTINGS.get("LLM_MODEL", "gpt-4o-mini")
    
    if model_name.startswith("gpt-"):
        return ChatOpenAI(
            model=model_name,
            temperature=0.7,
            streaming=streaming,  # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
```

## ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¡°

### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ êµ¬ì¡°

**í…Œì´ë¸”**: `chatbot_prompt`

| name | scope | content | tag | ìš©ë„ |
|------|-------|---------|-----|-----|
| llm_consultation | collection | í•œêµ­ì–´ ìƒë‹´ í”„ë¡¬í”„íŠ¸ | ko | LLM ëª¨ë“œ |
| llm_consultation_en | collection | ì˜ì–´ ìƒë‹´ í”„ë¡¬í”„íŠ¸ | en | LLM ëª¨ë“œ |
| llm_consultation_es | collection | ìŠ¤í˜ì¸ì–´ ìƒë‹´ í”„ë¡¬í”„íŠ¸ | es | LLM ëª¨ë“œ |
| rag_combine_answers_ko | collection | í•œêµ­ì–´ RAG ì¡°í•© í”„ë¡¬í”„íŠ¸ | ko | RAG ëª¨ë“œ |
| workpeople_zones | collection | ì§ì¥ì¸êµ¬ ê²€ìƒ‰ í”„ë¡¬í”„íŠ¸ | - | RAG ëª¨ë“œ |

### ëŒ€í™” ì €ì¥ êµ¬ì¡°

```python
# ChatMemory: ëŒ€í™” íˆìŠ¤í† ë¦¬
memory_type: 'question' | 'answer' | 'summary'
role: 'user' | 'assistant'
content: {"text": "ì‹¤ì œ ëŒ€í™” ë‚´ìš©"}

# CollectionMemory: RAG ê²€ìƒ‰ ê²°ê³¼
collection_name: "workpeople_zones"
retrieved_documents_content: ["ë¬¸ì„œ1", "ë¬¸ì„œ2", "ë¬¸ì„œ3"]
retrieved_documents_meta: [{"source": "..."}, ...]
llm_response: "ì»¬ë ‰ì…˜ë³„ ì‘ë‹µ"
```

## ğŸš€ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°

### WebSocket ë°ì´í„° íë¦„

```python
# RAG íŒŒì´í”„ë¼ì¸ ìŠ¤íŠ¸ë¦¬ë°
async for chunk in combine_answers_chain.astream(collection_answers):
    await self.send(text_data=json.dumps({"chunk": chunk}))

# LLM íŒŒì´í”„ë¼ì¸ ìŠ¤íŠ¸ë¦¬ë°  
async for chunk in llm_chain.astream({
    "question": question_with_summary,
    "chat_history": history_str
}):
    await self.send(text_data=json.dumps({"chunk": chunk}))

# ì™„ë£Œ ì‹ í˜¸
await self.send(text_data=json.dumps({
    "done": True,
    "session_id": self.session_id
}))
```

## âš™ï¸ ì„¤ì • ë° í™˜ê²½

### í™˜ê²½ë³€ìˆ˜

```bash
# OpenAI API
OPENAI_API_KEY=sk-...

# Qdrant ë²¡í„° DB
QDRANT_URL=https://your-qdrant-url
QDRANT_API_KEY=your-api-key

# Django ê¸°ë³¸
SECRET_KEY=your-secret-key
DEBUG=True
```

---

## ğŸ“ ìš”ì•½

LocaAI ì±—ë´‡ ì‹œìŠ¤í…œì€ **ëª¨ë“œ ê¸°ë°˜ ë¶„ê¸°**, **ë‹¤êµ­ì–´ ì§€ì›**, **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**ì„ í•µì‹¬ìœ¼ë¡œ í•˜ëŠ” í˜„ëŒ€ì ì¸ RAG ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.

**ì£¼ìš” íŠ¹ì§•**:
- ğŸ”€ **ë™ì  ëª¨ë“œ ì „í™˜**: LLM â†” RAG ì‹¤ì‹œê°„ í† ê¸€
- ğŸŒ **ë‹¤êµ­ì–´ ì§€ì›**: í•œêµ­ì–´/ì˜ì–´/ìŠ¤í˜ì¸ì–´ í”„ë¡¬í”„íŠ¸
- ğŸ“š **ë‹¤ì¤‘ ì»¬ë ‰ì…˜**: ë³‘ë ¬ ê²€ìƒ‰ ë° ì‘ë‹µ ì¡°í•©
- âš¡ **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°**: ì²­í¬ ë‹¨ìœ„ ì‘ë‹µ ì „ì†¡
- ğŸ§  **ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬**: ëŒ€í™” íˆìŠ¤í† ë¦¬ ë° ìš”ì•½ í™œìš©
- ğŸ”§ **í™•ì¥ ê°€ëŠ¥**: ìƒˆë¡œìš´ ì–¸ì–´/ì»¬ë ‰ì…˜ ì‰½ê²Œ ì¶”ê°€

ì´ ì•„í‚¤í…ì²˜ë¥¼ í†µí•´ ì‚¬ìš©ìëŠ” ìƒê¶Œ ë¶„ì„ ìƒë‹´ì— ìµœì í™”ëœ AI ì„œë¹„ìŠ¤ë¥¼ ë‹¤ì–‘í•œ ì–¸ì–´ì™€ ëª¨ë“œë¡œ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 